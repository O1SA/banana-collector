{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Banana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "%matplotlib inline\n",
    "from badaii import helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either using the library or the agent code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from badaii.rl.agents.double_dqn import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from badaii.rl.agents.replay_buffer import ReplayBuffer\n",
    "from badaii.moving_result import MovingResult\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Double DQN Agent with epsilon-greedy policy \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    seed: int \n",
    "        Number for random seeding\n",
    "    replay_size: int\n",
    "        Size of the experience replay buffer\n",
    "    batch_size: int\n",
    "        Size of the batch used when learning\n",
    "    gamma: float\n",
    "        Discount rate\n",
    "    lrate: int or float\n",
    "        Learning rate \n",
    "    tau: float\n",
    "        Soft target update rate\n",
    "    \"\"\"\n",
    "    def __init__(self, model=None, model_target=None, action_size=None, seed=None,\n",
    "                 replay_size = 100000, batch_size=64, update_frequency=4,\n",
    "                 gamma=0.99, lrate=5e-4, tau=0.001, restore=None):\n",
    "        if restore is not None:\n",
    "            checkpoint = torch.load(restore)\n",
    "            \n",
    "        self.update_frequency = update_frequency if not restore else checkpoint['update_frequency']\n",
    "        self.seed = seed if not restore else checkpoint['seed']\n",
    "        self.action_size = action_size if not restore else checkpoint['action_size']\n",
    "        self.replay_size = replay_size if not restore else checkpoint['replay_size']\n",
    "        self.batch_size = batch_size if not restore else checkpoint['batch_size']\n",
    "        self.gamma = gamma if not restore else checkpoint['gamma']\n",
    "        self.lrate = lrate if not restore else checkpoint['lrate']\n",
    "        self.tau = tau if not restore else checkpoint['tau']\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.q_network = model.to(self.device)\n",
    "        self.q_network_target = model_target.to(self.device)\n",
    "        \n",
    "        if restore is not None:\n",
    "            self.q_network.load_state_dict(checkpoint['state_dict'])\n",
    "            self.q_network_target.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.lrate)\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def save(self, filename):\n",
    "        checkpoint = self.params\n",
    "        checkpoint['state_dict'] = self.q_network.state_dict()\n",
    "        torch.save(checkpoint, filename)\n",
    "\n",
    "    def sample(self):\n",
    "        return np.random.randint(self.action_size)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.memory = ReplayBuffer(self.action_size, self.replay_size, self.batch_size, self.seed, self.device)\n",
    "        self.losses = MovingResult(100, name='loss', save_raw=False)\n",
    "        self.reset_episode()\n",
    "\n",
    "    def reset_episode(self):\n",
    "        self.it = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done, train=True):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        self.it += 1 \n",
    "        if train and self.it % self.update_frequency == 0 and len(self.memory) > self.batch_size:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences)\n",
    "    \n",
    "    def act(self, state, epsilon=0.):\n",
    "        \"\"\" Epsilon-Greedy policy\n",
    "        \n",
    "        \"\"\"\n",
    "        probs = epsilon * np.ones(self.action_size) / self.action_size\n",
    "        state = torch.from_numpy(state).float().to(self.device)\n",
    "        \n",
    "        self.q_network.eval()\n",
    "        with torch.no_grad():\n",
    "            probs[np.argmax(self.q_network(state).numpy())] += 1-epsilon\n",
    "        self.q_network.train()\n",
    "        return np.random.choice(np.arange(self.action_size), p=probs)\n",
    "    \n",
    "    def learn(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        # argmax_a Q^(next_state,a, w)\n",
    "        with torch.no_grad():\n",
    "            _ , best_actions = self.q_network(next_states).max(dim=1)\n",
    "            # y^ = td_target\n",
    "            # y^ = reward + gamma * Q^(next_state,argmax_a(next_state,a, w), w-), episode not terminal\n",
    "            # y^ = reward, episode terminal\n",
    "            td_targets = rewards + self.gamma * torch.gather(self.q_network_target(next_states),1,best_actions.view(-1,1))\n",
    "            for i in range(self.batch_size):\n",
    "                if dones[i].item() == 1.0:\n",
    "                    td_targets[i] = rewards[i]   \n",
    "        # delta = y^-Q\n",
    "        # clamp btwn -1..1\n",
    "        delta = torch.clamp(td_targets-torch.gather(self.q_network(states),1,actions), -1., 1.)\n",
    "        # loss = sum (y^-Q)^2\n",
    "        loss = torch.sum(torch.pow(delta,2))\n",
    "        self.losses.add(loss.item()/self.batch_size)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.soft_update()\n",
    "            \n",
    "    def soft_update(self):\n",
    "        \"\"\"Soft update of target network\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        \"\"\"\n",
    "        for target_param, param in zip(self.q_network_target.parameters(), self.q_network.parameters()):\n",
    "            target_param.data.copy_(self.tau*param.data+(1-self.tau)*target_param.data)\n",
    "            \n",
    "    @property\n",
    "    def params(self):\n",
    "        params = {\n",
    "            'state_size': self.state_size,\n",
    "            'action_size': self.action_size,\n",
    "            'seed': self.seed,\n",
    "            'replay_size': self.replay_size,\n",
    "            'batch_size': self.batch_size,\n",
    "            'gamma': self.gamma, \n",
    "            'lrate': self.lrate,\n",
    "            'tau': self.tau\n",
    "        }\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"data/VisualBanana.app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.visual_observations[0]\n",
    "print('States look like:')\n",
    "plt.imshow(np.squeeze(state))\n",
    "plt.show()\n",
    "state_size = state.shape\n",
    "print('States have shape:', state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play Random Game\n",
    "Watch an agent play randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.visual_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.visual_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_env(env, brain_name, train_mode=True):\n",
    "    env_info = env.reset(train_mode=train_mode)[brain_name]\n",
    "    return env_info.visual_observations[0].reshape(1,3,84,84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_env(env, brain_name, action):\n",
    "    env_info = env.step(action)[brain_name] \n",
    "    next_state = env_info.visual_observations[0].reshape(1,3,84,84)\n",
    "    reward = env_info.rewards[0] \n",
    "    done = env_info.local_done[0]\n",
    "    return (next_state, reward, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_replay_memory(agent, env, brain_name, size, steps = 2000, action_repeat=4):\n",
    "    it = 0 \n",
    "    while it < size: \n",
    "        state = reset_env(env, brain_name)\n",
    "        for step_i in range(steps):\n",
    "            action = agent.sample()\n",
    "            for _ in range(action_repeat):\n",
    "                next_state, reward, done = step_env(env, brain_name, action)\n",
    "            agent.step(state, action, reward, next_state, done, train=False)\n",
    "            it += 1 \n",
    "            state = next_state\n",
    "            if it >= size or done: \n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 32 8x8 filters with stride 4 _ Relu\n",
    "- 64 4x4 filters with stride 2 - ReLu\n",
    "- 64 3x3 filters with stride 1 - ReLu \n",
    "- Fully Connected Layer with 1024 units - ReLu\n",
    "- Fully Connected Layer with # of actions units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_channels, action_size, seed):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        self.state_channels = state_channels\n",
    "        self.action_size = action_size\n",
    "        self.seed = seed\n",
    "        \n",
    "        # 32 8x8 filters with stride 4\n",
    "        self.conv1 = nn.Conv2d(state_channels, 32, 8, stride=4)\n",
    "        # 64 4x4 filters with stride 2 \n",
    "        self.conv2 = nn.Conv2d(32,64,4, stride=2)\n",
    "        # 64 3x3 filters with stride 1 \n",
    "        self.conv3 = nn.Conv2d(64,64,3, stride=1)\n",
    "        # 64x7x7, 1024\n",
    "        self.fc1 = nn.Linear(64*7*7, 1024)\n",
    "        # 1024, action_size\n",
    "        self.fc2 = nn.Linear(1024, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc1(x.view(x.shape[0], -1)))\n",
    "        x = self.fc2(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 10\n",
    "steps = 2000\n",
    "out_file='model_v1.ckpt'\n",
    "print_every = 10\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "action_repeat = 4\n",
    "update_frequency = 4\n",
    "\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "learning_rate = 2.5e-4\n",
    "tau = 0.05\n",
    "\n",
    "replay_memory_size = 100000\n",
    "replay_start_size = replay_memory_size / 20\n",
    "\n",
    "initial_eps = 1.0\n",
    "final_eps = 0.1 \n",
    "final_exp_it = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = QNetwork(3,action_size,0)\n",
    "m_target = QNetwork(3, action_size, 0)\n",
    "\n",
    "agent = Agent(m, m_target, action_size=4, seed=0,\n",
    "              batch_size=batch_size,\n",
    "              gamma = gamma, \n",
    "              update_frequency = update_frequency,\n",
    "              lrate = learning_rate,\n",
    "              replay_size = replay_memory_size, \n",
    "              tau = tau\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reset()\n",
    "scores = helpers.MovingResult(100, name='score', save_raw=False)\n",
    "\n",
    "last_saved = 0 \n",
    "eps = initial_eps\n",
    "it = 0\n",
    "\n",
    "# fill replay memory \n",
    "init_replay_memory(agent, env, brain_name, replay_start_size, steps = 2000)\n",
    "\n",
    "with tnrange(episodes) as t: \n",
    "    for ep_i in t: \n",
    "        agent.reset_episode()\n",
    "        state = reset_env(env, brain_name)\n",
    "        score = 0\n",
    "        for step_i in range(steps):\n",
    "            action = agent.act(state, epsilon=eps)\n",
    "\n",
    "            # action repeat\n",
    "            for i in range(action_repeat):\n",
    "                next_state, reward, done = step_env(env, brain_name, action)\n",
    "\n",
    "            # step agent   \n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            score += reward\n",
    "            state = next_state\n",
    "            if done: \n",
    "                break \n",
    "            \n",
    "            # decay epsilon each it \n",
    "            it += 1 \n",
    "            eps = max(final_eps,initial_eps-(initial_eps-final_eps)/final_exp_it*it)\n",
    "        \n",
    "        # add episode score \n",
    "        scores.add(score)        \n",
    "        \n",
    "        # update log\n",
    "        if (ep_i+1) % print_every == 0:\n",
    "            t.set_postfix(it=it, epsilon=eps, score=scores.last)\n",
    "            \n",
    "        if scores.last >= 5.0 and scores.last > last_saved*1.05:\n",
    "            last_saved = scores.last\n",
    "            print('Saving...')\n",
    "            agent.save(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.losses.plot()\n",
    "scores.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "banana_0.4.0b",
   "language": "python",
   "name": "banana_0.4.0b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
